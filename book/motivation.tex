%% this is /book/motivation.tex
\section{Motivation}
\label{sec:motivation}
Due to the explosion in the amount of atmospheric, oceanographic,
climate and weather data either recorded \emph{in-situ} or generated
by modeling, inference and prediction algorithms, it is no longer
feasible for a single institution to host and maintain a centralized
database of information. Modern data management has been shifting
hosting and maintenance responsibilities of large datasets to multiple
participating institutions unified by a catalog service which provides
a single view of the distributed data to end users and
analysts~\cite{luettich13, williams09, chervenak00}. The institution
responsible for maintaining the catalog compiles meta data published
by registered participants regarding data hosted and exposed to the
catalog by the participant. Such federated datasets potentially span
petabytes of geospatial information, may be composed of millions of
files in different formats and are typically generated and hosted by
vastly different systems located across the globe. End users (such as
an analyst or scientist) interface with the catalog to search through
the aggregated meta data and interact with particular data of
interest, agnostic to distributed nature of the database.

Although a decentralized approach to data management offers many
advantages, data reduction and analysis tools have been slow to adapt
to the distributed framework. There exists an abundance of
applications for visualizing cartographic data on local computing
resources which require analysts to download local copies of datasets
and potentially reformat the data into the appropriate file format
before processing and analysis can begin. Even if an end user has
access to sufficient resources to download and process a dataset of
interest, tools designed for centralized-local systems increase
project costs in terms of bandwidth usage, time and
storage. Additionally, coupling centralized processing with
decentralized storage introduces the risk that different analysts
working with identical local copies of data obtained from the same
federation may use different local programs to generate incompatible
visualizations and reach conflicting conclusions. Normalizing these
results introduces a potential point of error and likewise increases
project costs in terms of time and accuracy.

%% HOW DOES SCIWMS SOLVE THESE PROBLEMS
%% CONTRIBUTIONS CAN GO HERE

By maintaining a list of web accessible data endpoints, \sciwms{} is
able to transparently produce consistent visualizations of web-hosted
data. While \sciwms{} implements the Open Geospatial Consortium (\ogc{})
Web Mapping Service (\wms{}) protocol~\cite{wms14}, it is augmented
with services for interacting with standard meta-data catalogs such as
Catalog Service for the Web (\csw{})~\cite{csw14}, allowing \sciwms{}
to autonomously track dynamic catalogs. 

\sciwms{} embraces distributed database principles and promotes the
separation of concerns software and project management practice. For
example, \sciwms{} may be deployed by a member of a much larger
project who's sole responsibility is providing a visualization
platform, promoting quality through specialization, saving time and
costs for data analysis projects by providing a simple interface for
end users to generate consistent visualizations of federated data.
